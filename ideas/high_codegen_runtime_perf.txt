Runtime Performance Improvements
=================================
Priority: HIGH

Remaining bottlenecks (profiled on zlib vs GCC -O2):

1. LOOP INDUCTION VARIABLE STRENGTH REDUCTION
   Array accesses in loops compute index*stride every iteration:
     movslq i, %rax; shlq $2, %rax; addq base, %rax; movl (%rax), ...
   Should increment a pointer instead: addq $4, %rdi; movl (%rdi), ...
   Fix: Loop strength reduction pass (iv_strength_reduce.rs exists but
   may need extension for pointer-based induction variables).

2. REDUNDANT SIGN EXTENSIONS (partially addressed)
   The lowering emits Cast i32->i64 for array indices even when the
   value is already 64-bit. Codegen emits redundant movslq/cltq.
   The peephole catches some but not all cases.
   DONE: Narrow pass now handles AShr/LShr shift narrowing (eliminates
   widen-shift-narrow pattern for right shifts, e.g. DOOM's fixed-point
   frac>>16 pattern). Reduced DOOM text by ~6KB, movslq by ~390.
   Remaining: Array index casts for pointer arithmetic (no narrowing
   cast follows, so the narrow pass can't help). Would need either IR-level
   analysis or backend-level elimination of redundant sign extensions.

3. REDUNDANT REGISTER-REGISTER MOVES
   Patterns like: movq %rax, %r14; movq %r14, %r15
   Arise because codegen routes through %rax as accumulator, then
   copies to callee-saved registers.
   Fix: Better peephole patterns, or teach the register allocator
   to avoid the accumulator roundtrip for register-to-register ops.

Key files:
- src/backend/regalloc.rs
- src/backend/liveness.rs
- src/backend/x86/codegen/peephole/
- src/passes/simplify.rs
- src/passes/iv_strength_reduce.rs
