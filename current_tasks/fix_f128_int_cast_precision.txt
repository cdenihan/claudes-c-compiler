Fix F128 integer cast precision loss on RISC-V and ARM

Problem: Converting 64-bit integers (signed/unsigned long long) to long double
(F128/binary128) loses mantissa precision on RISC-V and ARM. The classify_cast()
function in cast.rs reduces F128 to F64 for all operations, so int->F128 casts
go through an intermediate F64 conversion, losing bits beyond the 53-bit mantissa.

The result is that values like UINT64_MAX (2^64-1) become exactly 2^64, and
INT64_MAX (2^63-1) becomes 2^63. This is wrong because IEEE binary128 has a
113-bit mantissa, more than enough to represent all 64-bit integers exactly.

Fix:
1. Add new CastKind variants for F128<->integer casts
2. In RISC-V backend, emit calls to __floatditf/__floatunditf for int->F128
   and __fixtfdi/__fixunstfdi for F128->int
3. In ARM backend, same approach
4. Add regression test
