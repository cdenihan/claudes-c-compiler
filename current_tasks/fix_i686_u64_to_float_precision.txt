Task: Fix i686 unsigned 64-bit to floating-point conversion precision

Status: in_progress

Problem:
The i686 backend's U64 → F128 (long double), U64 → F64, and U64 → F32 conversions
used a shift-and-double trick for values >= 2^63 that lost precision. For x87 80-bit
long double (which has a 64-bit significand matching the input), this caused the
least significant bit to be lost. E.g. ULLONG_MAX (0xFFFFFFFFFFFFFFFF) converted
to 0xFFFFFFFFFFFFFFFE instead of the correct value.

Fix:
Replace the shift-and-double technique with the simpler and exact approach used by
GCC: fildq + conditional fadds of float constant 2^64 (0x5F800000). This is exact
for all target precisions because x87 performs the addition in 80-bit extended
precision before rounding to the target format.

Files modified:
- src/backend/i686/codegen/codegen.rs (UnsignedToF128, UnsignedToFloat U64→F64, U64→F32)
